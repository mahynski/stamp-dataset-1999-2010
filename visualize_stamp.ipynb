{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:59:20.973447Z",
     "start_time": "2021-05-10T15:59:20.954108Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:59:21.611354Z",
     "start_time": "2021-05-10T15:59:21.592473Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the raw STAMP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:56:33.770822Z",
     "start_time": "2021-05-10T16:56:33.721937Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://data.nist.gov/od/ds/mds2-2431/' # Raw data hosted here \n",
    "samples = pd.read_csv(url+'stamp_samples.csv') # Aliquot information\n",
    "chemistry = pd.read_csv(url+'stamp_chemistry.csv') # Result of chemical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:56:35.077782Z",
     "start_time": "2021-05-10T16:56:35.050365Z"
    }
   },
   "outputs": [],
   "source": [
    "chemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:56:38.197113Z",
     "start_time": "2021-05-10T16:56:38.170891Z"
    }
   },
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:56:46.418800Z",
     "start_time": "2021-05-10T16:56:46.400642Z"
    }
   },
   "outputs": [],
   "source": [
    "len(samples['GUSAMPLEID'].unique()) # Total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:56:46.823112Z",
     "start_time": "2021-05-10T16:56:46.801574Z"
    }
   },
   "outputs": [],
   "source": [
    "len(chemistry['analyte'].unique()) # Total number of unique analytes tested for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:56:47.320245Z",
     "start_time": "2021-05-10T16:56:47.304676Z"
    }
   },
   "outputs": [],
   "source": [
    "len(samples['GUALIQUOTID'].unique()) # Total number of aliquots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:56:48.293523Z",
     "start_time": "2021-05-10T16:56:48.015319Z"
    }
   },
   "outputs": [],
   "source": [
    "uaq = [len(samples[samples['GUSAMPLEID'] == s]['GUALIQUOTID'].unique()) for s in samples['GUSAMPLEID'].unique()]\n",
    "print(np.min(uaq), np.max(uaq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:57:22.001273Z",
     "start_time": "2021-05-10T16:57:18.258894Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the predominant species at each colony.\n",
    "\n",
    "# Get location of each colony\n",
    "lats = {c:set() for c in samples['COLONY_NAME'].unique()}\n",
    "lons = {c:set() for c in samples['COLONY_NAME'].unique()}\n",
    "for c in samples['COLONY_NAME'].unique():\n",
    "    lats[c] = lats[c].union(set(samples[samples['COLONY_NAME'] == c]['LATITUDE']))\n",
    "    lons[c] = lons[c].union(samples[samples['COLONY_NAME'] == c]['LONGITUDE'])\n",
    "\n",
    "# St. Lawrence  Isl. and St. George Isl. each have 3 slightly different sample locations, but are \n",
    "# essentially the same values so just average them for this purpose.\n",
    "locs = {c:(np.mean(list(lats[c])), np.mean(list(lons[c]))) for c in lats.keys()}\n",
    "\n",
    "plt.figure(figsize=(5,10))\n",
    "\n",
    "global_ = np.array([locs[c] for c in locs])\n",
    "source_proj = ccrs.PlateCarree()\n",
    "ax = plt.subplot(1,1,1, projection=source_proj)\n",
    "\n",
    "ax.stock_img()\n",
    "ax.set_extent((np.min(global_[:,1])-np.std(global_[:,1])*0.3, \n",
    "                   np.max(global_[:,1])+np.std(global_[:,1])*0.3,\n",
    "                   np.min(global_[:,0])-np.std(global_[:,0])*0.3, \n",
    "                   np.max(global_[:,0])+np.std(global_[:,0])*0.3)\n",
    "                 )\n",
    "    \n",
    "enc = LabelEncoder()\n",
    "enc.fit(np.unique(samples['COMMON_NAME']))\n",
    "\n",
    "for colony_, loc_ in locs.items():\n",
    "    top_bird = sorted(zip(*np.unique(samples[samples['COLONY_NAME'] == colony_]['COMMON_NAME'], return_counts=True)),\n",
    "           key=lambda x:x[1],\n",
    "           reverse=True\n",
    "          )[0][0]\n",
    "    color = enc.transform([top_bird])[0]\n",
    "    ax.plot(loc_[1], loc_[0], color='C{}'.format(color), linewidth=0, marker='o')\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color='C{}'.format(enc.transform([x])[0]), lw=4) \n",
    "               for x in enc.classes_]\n",
    "ax.legend(custom_lines, [x for x in enc.classes_], \n",
    "         bbox_to_anchor=(.8, .5))\n",
    "_ = ax.set_title('Predominant Species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:57:29.579591Z",
     "start_time": "2021-05-10T16:57:29.435820Z"
    }
   },
   "outputs": [],
   "source": [
    "chart = sns.catplot(y='GEOGRAPHIC_AREA_STAMP', kind=\"count\",\n",
    "            palette=\"pastel\", edgecolor=\".6\",\n",
    "            data=samples.drop_duplicates(subset='GUSAMPLEID', keep='first')\n",
    "                   )\n",
    "chart.set_ylabels('Geographic Area')\n",
    "chart.set_xlabels('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:57:34.585583Z",
     "start_time": "2021-05-10T16:57:34.408856Z"
    }
   },
   "outputs": [],
   "source": [
    "chart = sns.catplot(y='COLLECTION_YEAR', kind=\"count\",\n",
    "            palette=\"pastel\", edgecolor=\".6\",\n",
    "            data=samples.drop_duplicates(subset='GUSAMPLEID', keep='first'),\n",
    "            order=np.arange(1999, 2010+1),\n",
    "            )\n",
    "chart.set_ylabels('Collection Year')\n",
    "chart.set_xlabels('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:57:37.864982Z",
     "start_time": "2021-05-10T16:57:37.731043Z"
    }
   },
   "outputs": [],
   "source": [
    "s = samples.drop_duplicates(subset='GUSAMPLEID', keep='first')\n",
    "y = s['COMMON_NAME'] \n",
    "order = sorted(y.unique(), key=lambda x:x.split(' ')[1])\n",
    "\n",
    "fig,ax = plt.subplots(nrows=1, ncols=1,figsize=(5,7))\n",
    "ax.bar(\n",
    "    x=order,\n",
    "    height=[np.sum(y==c) for c in order],\n",
    ")\n",
    "plt.ylabel('Number of Observations', fontsize=12)\n",
    "_ = plt.xticks(rotation=90, fontsize=12)\n",
    "_ = plt.yticks(fontsize=12)\n",
    "\n",
    "for i,c in enumerate(order):\n",
    "    if (i < 3 or i > 4):\n",
    "        shift = .16\n",
    "    else:\n",
    "        shift=.2\n",
    "    plt.text(i-shift,np.sum(y==c)-10, str(np.sum(y==c)), color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge chemistry and samples into a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:57:43.110525Z",
     "start_time": "2021-05-10T16:57:43.086321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check which analytes co-eluted and need to be merged\n",
    "\n",
    "def custom_parser(to_merge):\n",
    "    def get(full_name, analyte_name):\n",
    "        multiples = [x.strip('and ') for x in full_name.split(',')] # No commas used in chemical name must be guaranteed\n",
    "        if (len(multiples) == 1): # Didn't split out any commas so just a pair\n",
    "            multiples = full_name.split(' and ')\n",
    "        return [' '.join([analyte_name, x.split(' ')[-1]]) for x in multiples]\n",
    "        \n",
    "    columns = []\n",
    "    for m in to_merge:\n",
    "        if m.startswith('BDEs'):\n",
    "            columns.append(get(m, 'BDE'))\n",
    "        elif m.startswith('PCBs'):\n",
    "            columns.append(get(m, 'PCB'))\n",
    "        else:\n",
    "            columns.append(m.split(' and '))\n",
    "    \n",
    "    return columns\n",
    "\n",
    "analytes_available = set(chemistry['analyte'].unique())\n",
    "to_merge = [x for x in analytes_available if 'and' in x] # Columns that need to be merged\n",
    "parsed = custom_parser(to_merge) # Individual columns split apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T17:00:02.809124Z",
     "start_time": "2021-05-10T16:57:50.137073Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "random.seed(0)\n",
    "\n",
    "# Can't merge on GUSAMPLEID because it doesn't exist in chemistry\n",
    "merged_inner = pd.merge(left=chemistry, \n",
    "                        right=samples, \n",
    "                        left_on='GUALIQUOTID', \n",
    "                        right_on='GUALIQUOTID')\n",
    "\n",
    "# Sanity check that individual aliquots come from same sample\n",
    "for al_id in merged_inner['GUALIQUOTID'].unique(): \n",
    "    assert(len(merged_inner[merged_inner['GUALIQUOTID'] == al_id]['GUSAMPLEID'].unique()) == 1)\n",
    "    \n",
    "X,y = None, None\n",
    "\n",
    "for i,sa_id in tqdm.tqdm(enumerate(merged_inner['GUSAMPLEID'].unique())):\n",
    "    # For each sample ID, find all aliquots and combine results\n",
    "    this_sample = merged_inner[merged_inner['GUSAMPLEID'] == sa_id]\n",
    "    frames = []\n",
    "    for al_id in this_sample['GUALIQUOTID'].unique():\n",
    "        frames.append(this_sample[this_sample['GUALIQUOTID'] == al_id])\n",
    "    combined = pd.concat(frames, axis=0, sort=True, verify_integrity=True, ignore_index=True)\n",
    "    \n",
    "    # Check analytes not duplicated\n",
    "    u_, c_ = np.unique(combined['analyte'], return_counts=True)\n",
    "    if (np.any(c_ > 1)):\n",
    "        print('duplicates found for {} {} {}'.format(i, sa_id, combined['COMMON_NAME'].iloc[0]))\n",
    "        combined.drop_duplicates(subset=['analyte'], keep='first', inplace=True)\n",
    "\n",
    "    # Merge, impute, etc.\n",
    "    for merged, aset in zip(to_merge, parsed): \n",
    "        if (merged not in combined['analyte'].values):\n",
    "            # Sum was not directly tested, try to construct it\n",
    "            isum = 0.0\n",
    "            rand = 0.0\n",
    "            for analyte in aset:\n",
    "                if analyte in combined['analyte'].values:\n",
    "                    v = combined[combined['analyte'] == analyte]['value'].values[0]\n",
    "                    isum += (0.0 if np.isnan(v) else v) # Non-detects impute to 0, according to Jared\n",
    "                    if (np.isnan(v)):\n",
    "                        assert(combined[combined['analyte'] == analyte]['dl'].values[0] > 0), 'check dL are > 0'\n",
    "                        rand += (random.random()*combined[combined['analyte'] == analyte]['dl'].values[0]) # dL should be present if value is not, so this is only a float if all measurements were NaN (below dL)\n",
    "                else:\n",
    "                    isum = np.nan # Wasn't even measured, can't build sum\n",
    "                    break\n",
    "            \n",
    "            if isum == 0.0: # If all were tested but undetected \n",
    "                new_row = pd.DataFrame([[np.nan] * combined.shape[1]], \n",
    "                                       columns=combined.columns)\n",
    "                new_row['value'] = rand\n",
    "                new_row['analyte'] = merged\n",
    "                combined = pd.concat([combined, new_row], ignore_index=True)\n",
    "            else: \n",
    "                # If some are measured, report net as sum of those detected\n",
    "                # In the case that one or more terms missing this becomes NaN (see above) as we cannot construct this some\n",
    "                new_row = pd.DataFrame([[np.nan] * combined.shape[1]], \n",
    "                                       columns=combined.columns)\n",
    "                new_row['value'] = isum\n",
    "                new_row['analyte'] = merged\n",
    "                combined = pd.concat([combined, new_row], ignore_index=True)\n",
    "        else:\n",
    "            # Sum was directly tested for\n",
    "            if np.isnan(combined[combined['analyte'] == merged]['value'].values[0]):\n",
    "                # Below dL, impute to 0 < RNG < dL\n",
    "                assert(combined[combined['analyte'] == merged]['dl'].values[0] > 0), 'check dL are > 0'\n",
    "                v = random.random()*combined[combined['analyte'] == merged]['dl'].values[0]\n",
    "                combined.loc[combined[combined['analyte'] == merged].index, 'value'] = v\n",
    "            else:\n",
    "                # Above dL, nothing to do\n",
    "                pass\n",
    "            \n",
    "        # Remove individual terms\n",
    "        for analyte in aset:\n",
    "            combined = combined.drop(index=combined[combined['analyte'] == analyte].index)\n",
    "        \n",
    "    # For non-detects that are not going to be summed, impute to 0 < RNG < dL\n",
    "    # Skip analytes that were just merged and those which had to be removed\n",
    "    for analyte in [a for a in analytes_available if a not in to_merge+list(chain.from_iterable(parsed))]: \n",
    "        if analyte in combined['analyte'].values:\n",
    "            # Analyte was tested for - if below dL, impute 0 < RNG < dL\n",
    "            v = combined[combined['analyte'] == analyte]['value'].values[0]\n",
    "            if np.isnan(v):\n",
    "                assert(combined[combined['analyte'] == analyte]['dl'].values[0] > 0), 'check dL are > 0'\n",
    "                v = random.random()*combined[combined['analyte'] == analyte]['dl']\n",
    "                combined.loc[combined[combined['analyte'] == analyte].index, 'value'] = v\n",
    "        else:\n",
    "            # Have to add a NaN (not tested) for this analyte\n",
    "            new_row = pd.DataFrame([[np.nan] * combined.shape[1]], \n",
    "                                       columns=combined.columns)\n",
    "            new_row['analyte'] = analyte\n",
    "            combined = pd.concat([combined, new_row], ignore_index=True)\n",
    "\n",
    "    # 2. Store targets\n",
    "    target_df = combined[['COLONY_NAME', 'COLLECTION_YEAR', 'COMMON_NAME', 'GUSAMPLEID']]\n",
    "    for t in target_df.columns: # Assert that each data point has only one name, year, etc.\n",
    "        assert(target_df[t].nunique() == 1), '{} {}'.format(t, target_df[t].nunique())\n",
    "        \n",
    "    # 3. Convert first row ('analyte') into header and remove as row\n",
    "    raw = combined[['analyte', 'value']].transpose()\n",
    "    raw.columns = raw.iloc[0]\n",
    "    raw = raw.iloc[pd.RangeIndex(len(raw)).drop(0)]\n",
    "        \n",
    "    X = pd.concat([X, raw], axis=0, sort=True, verify_integrity=True,  \n",
    "                    ignore_index=True\n",
    "                    ) \n",
    "    y = pd.concat([y, target_df[:1]], axis=0, sort=True, verify_integrity=True,\n",
    "                    ignore_index=True)\n",
    "    \n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T17:03:33.569296Z",
     "start_time": "2021-05-10T17:03:33.552013Z"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set minimum observation threshold to 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T17:03:36.002129Z",
     "start_time": "2021-05-10T17:03:35.979135Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.95 # Set that analytes must be measured at least this % of the time\n",
    "chosen = X.columns[X.isnull().sum() < X.shape[0]*(1.0-threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T17:03:36.885220Z",
     "start_time": "2021-05-10T17:03:36.864026Z"
    }
   },
   "outputs": [],
   "source": [
    "data = X[chosen].join(y, how='outer').dropna() # Remove NaN entries since they correspond to test not performeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T17:03:38.062259Z",
     "start_time": "2021-05-10T17:03:38.041591Z"
    }
   },
   "outputs": [],
   "source": [
    "used = data[data['COMMON_NAME'] != 'Unknown murre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T17:03:41.176409Z",
     "start_time": "2021-05-10T17:03:41.142823Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these data will vary slightly relative to that used due to the generation of random numbers to\n",
    "interpolate below detection limits.  Even if the random number generator's seed is set, the use of \n",
    "unorder sets in the above loop means things may be accessed or ordered at certain stages in a machine-dependent manner.\n",
    "\n",
    "This overall results are insensitive to this randomness since these generated numbers only contributed to things below the detection limit, and had only marginal quantitative impact, and no qualitative impact, on any of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T17:03:44.857718Z",
     "start_time": "2021-05-10T17:03:44.838975Z"
    }
   },
   "outputs": [],
   "source": [
    "features = [c for c in used.columns if c not in {\"COLLECTION_YEAR\", \"COLONY_NAME\", \"COMMON_NAME\", \"GUSAMPLEID\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T17:04:04.376253Z",
     "start_time": "2021-05-10T17:04:04.179945Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "ss = StandardScaler()\n",
    "X_proj = pca.fit_transform(ss.fit_transform(used[features]))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "for bird in np.unique(y['COMMON_NAME']):\n",
    "    mask = used['COMMON_NAME'] == bird\n",
    "    plt.plot(X_proj[mask,0], X_proj[mask,1], 'o', label=bird)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('PC 1 ({}%)'.format('%.1f'%(100*pca.explained_variance_ratio_[0])))\n",
    "plt.ylabel('PC 2 ({}%)'.format('%.1f'%(100*pca.explained_variance_ratio_[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T17:04:10.273962Z",
     "start_time": "2021-05-10T17:04:10.204568Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "ss = StandardScaler()\n",
    "X_proj = pca.fit_transform(ss.fit_transform(used[features]))\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for bird in np.unique(used['COMMON_NAME']):\n",
    "    mask = used['COMMON_NAME'] == bird \n",
    "    ax.scatter(X_proj[mask,0], X_proj[mask,1], X_proj[mask,2], label=bird)\n",
    "ax.set_xlabel('PC 1 ({}%)'.format('%.1f'%(100*pca.explained_variance_ratio_[0])))\n",
    "ax.set_ylabel('PC 2 ({}%)'.format('%.1f'%(100*pca.explained_variance_ratio_[1])))\n",
    "ax.set_zlabel('PC 3 ({}%)'.format('%.1f'%(100*pca.explained_variance_ratio_[2])))\n",
    "ax.view_init(elev=20., azim=-45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T17:04:12.612183Z",
     "start_time": "2021-05-10T17:04:12.195849Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "pca_comps = sorted(zip(pca.components_[0], features), key=lambda x:np.abs(x[0]), reverse=True)\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.bar(\n",
    "    x=[a_[1] for a_ in pca_comps],\n",
    "    height=np.abs([a_[0] for a_ in pca_comps]),\n",
    ")\n",
    "_ = plt.xticks(rotation=90)\n",
    "plt.ylabel('|Coefficient| in PC 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
